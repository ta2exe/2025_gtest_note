<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>1-6-4 深層強化学習</title>
  <!-- 絵文字ファビコン -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🤖</text></svg>">
  
  <!-- フォントの読み込み -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Zen+Maru+Gothic:wght@400;500;700&family=Klee+One:wght@400;600&family=M+PLUS+Rounded+1c:wght@400;500;700&display=swap">
  
  <!-- FontAwesome -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  
  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    :root {
      /* CMYK カラーパレット */
      --cyan: #00D8E8;
      --magenta: #FF40A0;
      --yellow: #FFE600;
      --key: #181818;
      --dark-gray: #404040;
      --white: #FFFFFF;
      
      /* フォント設定 */
      --main-font: 'Zen Maru Gothic', sans-serif;
      --title-font: 'M PLUS Rounded 1c', sans-serif;
      --handwritten-font: 'Klee One', cursive;
    }
    
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    body {
      font-family: var(--main-font);
      background-color: var(--white);
      color: var(--key);
      line-height: 1.6;
      display: flex;
      min-height: 100vh;
    }
    
    /* レイアウト */
    .sidebar {
      width: 20%;
      background-color: #000000;
      color: var(--white);
      padding: 2rem 1rem;
      position: fixed;
      height: 100vh;
      overflow-y: auto;
      box-shadow: 2px 0 10px rgba(0, 0, 0, 0.1);
    }
    
    .content {
      width: 80%;
      margin-left: 20%;
      padding: 2rem;
    }
    
    /* スクロールバーのデザイン */
    .sidebar::-webkit-scrollbar {
      width: 8px;
    }
    
    .sidebar::-webkit-scrollbar-track {
      background: rgba(255, 255, 255, 0.1);
    }
    
    .sidebar::-webkit-scrollbar-thumb {
      background: var(--cyan);
      border-radius: 4px;
    }
    
    /* ヘッダー */
    .header {
      padding: 2rem 0;
      margin-bottom: 2rem;
      text-align: center;
      border-bottom: 2px dashed var(--cyan);
    }
    
    .title {
      font-family: var(--title-font);
      font-size: 2.5rem;
      font-weight: 700;
      color: var(--key);
      margin-bottom: 1rem;
    }
    
    .subtitle {
      font-family: var(--handwritten-font);
      font-size: 1.5rem;
      color: var(--magenta);
      margin-bottom: 1.5rem;
    }
    
    /* セクション */
    section {
      margin-bottom: 4rem;
      padding: 2rem;
      background-color: rgba(255, 255, 255, 0.7);
      border-radius: 12px;
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.05);
      position: relative;
      overflow: hidden;
    }
    
    section::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 5px;
      background: linear-gradient(to right, var(--cyan), var(--magenta), var(--yellow));
    }
    
    h2 {
      font-family: var(--title-font);
      font-size: 1.8rem;
      color: var(--key);
      margin-bottom: 1rem;
      padding-bottom: 0.5rem;
      border-bottom: 2px solid var(--magenta);
    }
    
    h3 {
      font-family: var(--handwritten-font);
      font-size: 1.5rem;
      color: var(--dark-gray);
      margin: 1.5rem 0 1rem;
      padding-left: 1rem;
      border-left: 4px solid var(--cyan);
    }
    
    p {
      margin-bottom: 1rem;
      font-size: 1.1rem;
    }
    
    /* サイドバー */
    .sidebar-title {
      font-family: var(--title-font);
      font-size: 1.3rem;
      font-weight: 700;
      color: var(--yellow);
      text-align: center;
      margin-bottom: 2rem;
      padding-bottom: 1rem;
      border-bottom: 1px solid var(--cyan);
    }
    
    .sidebar-nav {
      margin-bottom: 2rem;
    }
    
    .nav-title {
      font-size: 1.1rem;
      font-weight: 600;
      color: var(--cyan);
      margin-bottom: 0.8rem;
    }
    
    .nav-links {
      list-style-type: none;
      padding-left: 0.5rem;
    }
    
    .nav-links li {
      margin-bottom: 0.8rem;
    }
    
    .nav-links a {
      color: var(--white);
      text-decoration: none;
      font-size: 0.95rem;
      display: flex;
      align-items: center;
      transition: all 0.3s ease;
    }
    
    .nav-links a:hover {
      color: var(--yellow);
      transform: translateX(5px);
    }
    
    .nav-links a i {
      margin-right: 8px;
      color: var(--magenta);
    }
    
    .active {
      color: var(--yellow) !important;
      font-weight: bold;
    }
    
    /* コンテンツボックス */
    .content-box {
      background-color: var(--white);
      border-radius: 8px;
      padding: 1.5rem;
      margin-bottom: 2rem;
      box-shadow: 0 4px 10px rgba(0, 0, 0, 0.05);
    }
    
    /* ハイライト要素 */
    .highlight {
      background-color: rgba(255, 230, 0, 0.2);
      padding: 0.2rem 0.4rem;
      border-radius: 4px;
      font-weight: 500;
    }
    
    .key-term {
      color: var(--magenta);
      font-weight: 700;
      position: relative;
      display: inline-block;
    }
    
    .key-term::after {
      content: '';
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: 3px;
      background-color: var(--cyan);
      opacity: 0.5;
    }
    
    /* ノートボックス */
    .note-box {
      border-left: 5px solid var(--magenta);
      padding: 1rem 1.5rem;
      margin: 1.5rem 0;
      background-color: rgba(255, 64, 160, 0.05);
      border-radius: 0 8px 8px 0;
      position: relative;
    }
    
    .note-box::before {
      content: '📌';
      position: absolute;
      top: -0.8rem;
      left: -0.8rem;
      font-size: 1.5rem;
    }
    
    /* 手書き風ボックス */
    .handwritten-box {
      font-family: var(--handwritten-font);
      padding: 1.5rem;
      margin: 1.5rem 0;
      background-color: rgba(0, 216, 232, 0.05);
      border: 2px dashed var(--cyan);
      border-radius: 12px;
      position: relative;
      transform: rotate(-0.5deg);
    }
    
    /* 吹き出し */
    .speech-bubble {
      position: relative;
      padding: 1.5rem;
      margin: 2rem 0;
      background-color: var(--white);
      border-radius: 10px;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    }
    
    .speech-bubble::after {
      content: '';
      position: absolute;
      bottom: -15px;
      left: 50px;
      border-width: 15px 15px 0;
      border-style: solid;
      border-color: var(--white) transparent;
    }
    
    /* 画像コンテナ */
    .image-container {
      margin: 1.5rem 0;
      text-align: center;
      max-width: 100%;
    }
    
    .image-container img {
      max-width: 75%;
      height: auto;
      border-radius: 8px;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
      border: 1px solid #e0e0e0;
    }
    
    .image-container figcaption {
      margin-top: 0.8rem;
      font-size: 0.9rem;
      color: var(--dark-gray);
      font-style: italic;
      text-align: center;
      padding: 0 10%;
      line-height: 1.5;
      border-bottom: 1px dashed var(--cyan);
      padding-bottom: 0.5rem;
      display: inline-block;
    }
    
    /* リスト */
    ul, ol {
      margin: 1rem 0 1.5rem 1.5rem;
      list-style-position: inside;
    }
    
    li {
      margin-bottom: 0.5rem;
    }
    
    /* 表 */
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1.5rem 0;
    }
    
    th, td {
      padding: 0.8rem;
      border: 1px solid #e0e0e0;
    }
    
    th {
      background-color: rgba(0, 216, 232, 0.1);
      font-weight: 600;
    }
    
    tr:nth-child(even) {
      background-color: rgba(255, 255, 255, 0.5);
    }
    
    /* Key Insights セクション */
    .key-insights {
      margin-top: 3rem;
      padding: 2rem;
      background-color: rgba(255, 64, 160, 0.05);
      border-radius: 12px;
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.05);
    }
    
    .key-insights h2 {
      color: var(--magenta);
      border-bottom: 2px solid var(--yellow);
    }
    
    .insights-list {
      list-style-type: none;
    }
    
    .insights-list li {
      margin-bottom: 1.5rem;
      padding-left: 2.5rem;
      position: relative;
    }
    
    .insights-list li::before {
      content: '💡';
      position: absolute;
      left: 0;
      top: 0;
      font-size: 1.5rem;
    }
    
    /* Take Home Message セクション */
    .take-home {
      margin-top: 3rem;
      padding: 2rem;
      background-color: rgba(255, 230, 0, 0.1);
      border-radius: 12px;
      text-align: center;
      position: relative;
    }
    
    .take-home h2 {
      color: var(--key);
      border-bottom: 2px solid var(--yellow);
      display: inline-block;
    }
    
    .take-home-message {
      font-family: var(--handwritten-font);
      font-size: 1.4rem;
      font-weight: 600;
      margin: 1.5rem 0;
      color: var(--key);
      line-height: 1.6;
    }
    
    /* フッター */
    footer {
      margin-top: 4rem;
      padding-top: 2rem;
      border-top: 1px solid #e0e0e0;
      text-align: center;
      font-size: 0.9rem;
      color: var(--dark-gray);
    }
    
    /* 用語集 */
    .glossary {
      margin-top: 3rem;
    }
    
    .glossary-table {
      width: 100%;
      border-collapse: collapse;
    }
    
    .glossary-table th {
      background-color: rgba(0, 216, 232, 0.2);
      color: var(--key);
    }
    
    .glossary-term {
      font-weight: 700;
      color: var(--key);
    }
    
    .glossary-en {
      font-style: italic;
      color: var(--magenta);
      font-size: 0.9rem;
    }
    
    /* スクロールトップボタン */
    .scroll-top {
      position: fixed;
      bottom: 2rem;
      right: 2rem;
      width: 50px;
      height: 50px;
      background-color: var(--cyan);
      color: var(--white);
      border-radius: 50%;
      display: flex;
      justify-content: center;
      align-items: center;
      font-size: 1.5rem;
      cursor: pointer;
      box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2);
      transition: all 0.3s ease;
      opacity: 0;
      visibility: hidden;
    }
    
    .scroll-top.visible {
      opacity: 1;
      visibility: visible;
    }
    
    .scroll-top:hover {
      transform: translateY(-5px);
    }
    
    /* レスポンシブ対応 */
    @media screen and (max-width: 992px) {
      .sidebar {
        width: 240px;
        transform: translateX(-100%);
        z-index: 1000;
        transition: transform 0.3s ease;
      }
      
      .sidebar.active {
        transform: translateX(0);
      }
      
      .content {
        width: 100%;
        margin-left: 0;
      }
      
      .menu-toggle {
        position: fixed;
        top: 1rem;
        left: 1rem;
        width: 40px;
        height: 40px;
        background-color: var(--cyan);
        color: var(--white);
        border-radius: 50%;
        display: flex;
        justify-content: center;
        align-items: center;
        font-size: 1.2rem;
        cursor: pointer;
        z-index: 1001;
        box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2);
      }
      
      .title {
        font-size: 2rem;
      }
      
      .subtitle {
        font-size: 1.2rem;
      }
    }
    
    @media screen and (max-width: 768px) {
      section {
        padding: 1.5rem;
      }
      
      .title {
        font-size: 1.8rem;
      }
      
      h2 {
        font-size: 1.5rem;
      }
      
      h3 {
        font-size: 1.3rem;
      }
      
      .image-container img {
        max-width: 100%;
      }
    }
  </style>
</head>
<body>
  <!-- サイドバー -->
  <aside class="sidebar">
    <div class="sidebar-title">
      <i class="fas fa-robot"></i> ディープラーニングの応用例
    </div>
    <nav class="sidebar-nav">
      <div class="nav-title">
        <i class="fas fa-list"></i> 目次
      </div>
      <ul class="nav-links">
        <li><a href="#introduction"><i class="fas fa-rocket"></i> 深層強化学習の概要</a></li>
        <li><a href="#models"><i class="fas fa-cube"></i> 代表的なモデル</a></li>
        <li><a href="#applications"><i class="fas fa-cogs"></i> 実世界での活用</a></li>
        <li><a href="#key-algorithms"><i class="fas fa-microchip"></i> 主要アルゴリズム</a></li>
        <li><a href="#glossary"><i class="fas fa-book"></i> 用語集</a></li>
        <li><a href="#key-insights"><i class="fas fa-lightbulb"></i> Key Insights</a></li>
        <li><a href="#take-home"><i class="fas fa-key"></i> Take Home Message</a></li>
      </ul>
    </nav>
  </aside>

  <!-- メインコンテンツ -->
  <main class="content">
    <!-- ヘッダー -->
    <header class="header">
      <h1 class="title">1-6-4 深層強化学習</h1>
      <p class="subtitle">Deep Reinforcement Learning</p>
      <div class="content-box">
        <p>このセクションでは、深層強化学習の概要、代表的なモデル、そして実世界での活用事例について学びます。深層強化学習は、強化学習とディープラーニングを組み合わせた技術であり、ゲームからロボット制御まで様々な複雑な問題に応用されています。</p>
      </div>
    </header>

    <!-- コンテンツセクション -->
    <section id="introduction">
      <h2><i class="fas fa-rocket"></i> 深層強化学習の概要</h2>
      
      <p>深層強化学習（Deep Reinforcement Learning）は、強化学習の枠組みにディープラーニングを組み合わせたアプローチです。従来の強化学習では扱いきれなかった複雑な状態空間を持つ問題に対して、ニューラルネットワークの強力な表現力を活用します。</p>
      
      <div class="handwritten-box">
        <h3>深層強化学習の基本要素</h3>
        <ul>
          <li><span class="key-term">状態表現</span>：ディープニューラルネットワークを用いて環境の状態を表現</li>
          <li><span class="key-term">方策関数</span>：状態から行動を決定する関数（ニューラルネットで実装）</li>
          <li><span class="key-term">価値関数</span>：状態や行動の価値を評価する関数（ニューラルネットで実装）</li>
          <li><span class="key-term">報酬信号</span>：エージェントの行動に対するフィードバック</li>
        </ul>
      </div>
      
      <h3>強化学習と深層強化学習の違い</h3>
      <div class="content-box">
        <table>
          <tr>
            <th>従来の強化学習</th>
            <th>深層強化学習</th>
          </tr>
          <tr>
            <td>テーブルや線形関数で表現</td>
            <td>ディープニューラルネットワークで表現</td>
          </tr>
          <tr>
            <td>離散的で小規模な状態空間に適用</td>
            <td>連続的で大規模な状態空間に適用可能</td>
          </tr>
          <tr>
            <td>手作業による特徴量設計が必要</td>
            <td>生データからの特徴抽出が可能</td>
          </tr>
          <tr>
            <td>計算効率が高い</td>
            <td>計算リソースを多く必要とする</td>
          </tr>
        </table>
      </div>
      
      <div class="note-box">
        <p>深層強化学習の特徴は「<span class="highlight">エンド・ツー・エンド学習</span>」にあります。入力された生データから直接適切な行動を出力できるため、人間による特徴量の設計が不要になります。これにより、画像や音声などの複雑な入力を扱う問題に適用できるようになりました。</p>
      </div>
      
      <h3>深層強化学習の学習プロセス</h3>
      <ol>
        <li>環境から状態を観測</li>
        <li>ニューラルネットワークを用いて状態を処理し行動を選択</li>
        <li>行動を実行し、環境から報酬と次の状態を受け取る</li>
        <li>得られた経験から学習（ニューラルネットワークのパラメータ更新）</li>
        <li>最適な方策を学習するまで繰り返す</li>
      </ol>
      
      <div class="speech-bubble">
        <p>深層強化学習の登場により、従来の強化学習では難しかった「<span class="key-term">生の視覚情報からの行動学習</span>」や「<span class="key-term">長期的な計画立案</span>」が可能になりました。これが、<span class="key-term">Atari</span>ゲームや<span class="key-term">AlphaGo</span>といった成功事例につながっています。</p>
      </div>
    </section>

    <!-- コンテンツセクション - 代表的なモデル -->
    <section id="models">
      <h2><i class="fas fa-cube"></i> 代表的なモデル</h2>
      
      <p>深層強化学習の発展過程で、様々な革新的なモデルが提案されてきました。ここでは、特に重要で広く知られているモデルについて学びます。</p>
      
      <h3>代表的な深層強化学習モデル</h3>
      
      <div class="content-box">
        <h3><i class="fas fa-gamepad"></i> DQN (Deep Q-Network)</h3>
        <p><span class="key-term">DQN</span> (Deep Q-Network) は、2013年にDeepMindによって発表された深層強化学習の先駆的モデルです。Q学習とディープニューラルネットワークを組み合わせ、Atariゲームで人間レベルのパフォーマンスを達成しました。</p>
        
        <div class="handwritten-box">
          <h4>DQNの革新的技術</h4>
          <ul>
            <li><span class="key-term">経験リプレイ</span>：過去の経験をランダムに再利用して学習の安定性を向上</li>
            <li><span class="key-term">ターゲットネットワーク</span>：学習の安定化のための二重ネットワーク構造</li>
            <li><span class="key-term">CNN活用</span>：画面ピクセルからの直接学習を実現</li>
          </ul>
        </div>
      </div>
      
      <div class="content-box">
        <h3><i class="fas fa-chess"></i> アルファ碁 (AlphaGo) と派生モデル</h3>
        <p><span class="key-term">アルファ碁（AlphaGo）</span>は2016年に人間のプロ棋士に勝利し、世界中で注目を集めました。その後も進化を続け、<span class="key-term">AlphaGo Zero</span>、<span class="key-term">AlphaZero</span>、<span class="key-term">MuZero</span>と発展しています。</p>
        
        <table>
          <tr>
            <th>モデル</th>
            <th>特徴</th>
            <th>革新点</th>
          </tr>
          <tr>
            <td>AlphaGo</td>
            <td>教師あり学習と強化学習の組み合わせ</td>
            <td>モンテカルロ木探索と深層学習の融合</td>
          </tr>
          <tr>
            <td>AlphaGo Zero</td>
            <td>人間のデータを一切使わずに学習</td>
            <td>自己対戦による完全な自己学習</td>
          </tr>
          <tr>
            <td>AlphaZero</td>
            <td>囲碁、将棋、チェスに適用可能</td>
            <td>汎用的なアルゴリズム設計</td>
          </tr>
          <tr>
            <td>MuZero</td>
            <td>ルールを知らなくても学習可能</td>
            <td>環境モデルの学習を統合</td>
          </tr>
        </table>
      </div>
      
      <div class="content-box">
        <h3><i class="fas fa-robot"></i> PPO (Proximal Policy Optimization)</h3>
        <p><span class="key-term">PPO</span> (Proximal Policy Optimization) は、OpenAIによって2017年に提案された、安定性と実装の容易さを両立したアルゴリズムです。多くの実用的なアプリケーションで採用されています。</p>
        
        <div class="note-box">
          <p>PPOの最大の特徴は<span class="highlight">安定した学習</span>にあります。方策の更新幅を制限することで、学習の破綻を防ぎつつ効率的な改善を実現しています。また、実装が比較的簡単で調整も容易なため、実践的なアプリケーションでよく使用されています。</p>
        </div>
      </div>
      
      <div class="content-box">
        <h3><i class="fas fa-users"></i> マルチエージェントと協調学習</h3>
        <p>複数のエージェントが協調または競争する環境での深層強化学習も重要な研究分野です。<span class="key-term">OpenAI Five</span>はDota 2というチームベースのゲームで人間チームに勝利し、<span class="key-term">アルファスター（AlphaStar）</span>はスタークラフトIIというリアルタイム戦略ゲームでプロプレイヤーを上回りました。</p>
        
        <div class="handwritten-box">
          <h4>マルチエージェント学習の特徴</h4>
          <ul>
            <li>複数のエージェントの相互作用を考慮</li>
            <li>協調戦略や対抗戦略の学習</li>
            <li>不完全情報下での意思決定</li>
            <li>スケーラブルな学習アルゴリズム</li>
          </ul>
        </div>
      </div>
      
      <div class="speech-bubble">
        <p>深層強化学習モデルの発展は非常に速く、DQNからわずか数年でAlphaGoやPPOなどの革新的なアルゴリズムが次々と登場しました。これらのモデルは単なる技術的進歩を超えて、AIの可能性と限界に関する私たちの理解を大きく変えています。</p>
      </div>
    </section>

    <!-- コンテンツセクション - 実世界での活用 -->
    <section id="applications">
      <h2><i class="fas fa-cogs"></i> 実世界での活用</h2>
      
      <p>深層強化学習はゲームだけでなく、様々な実世界の問題に応用されています。その適用範囲は日々拡大しており、産業界でも積極的に活用されるようになっています。</p>
      
      <h3>産業応用領域</h3>
      
      <div class="content-box">
        <h3><i class="fas fa-industry"></i> ロボティクスと自動制御</h3>
        <p>深層強化学習は複雑な物理環境におけるロボットの制御に応用されています。現実世界の複雑さに対応するために、<span class="key-term">sim2real</span>（シミュレーションから現実への転移）や<span class="key-term">ドメインランダマイゼーション</span>などの技術が開発されています。</p>
        
        <div class="handwritten-box">
          <h4>主なロボティクス応用例</h4>
          <ul>
            <li>ロボットアームによる物体把持・操作</li>
            <li>二足歩行ロボットの歩行制御</li>
            <li>ドローンの自律飛行</li>
            <li>自動車の自動運転支援</li>
          </ul>
        </div>
      </div>
      
      <div class="content-box">
        <h3><i class="fas fa-chart-line"></i> ビジネスと最適化</h3>
        <p>企業活動における複雑な意思決定問題にも深層強化学習が応用されています。特に、多変数の最適化問題や長期的な戦略決定に効果を発揮します。</p>
        
        <table>
          <tr>
            <th>応用分野</th>
            <th>具体例</th>
          </tr>
          <tr>
            <td>広告配信最適化</td>
            <td>ユーザーの長期的なエンゲージメントを最大化する広告表示</td>
          </tr>
          <tr>
            <td>在庫・物流管理</td>
            <td>需要予測に基づく在庫最適化や配送ルート最適化</td>
          </tr>
          <tr>
            <td>価格設定</td>
            <td>需要と供給のバランスを考慮した動的価格設定</td>
          </tr>
          <tr>
            <td>エネルギー管理</td>
            <td>電力需要予測と発電量の最適化</td>
          </tr>
        </table>
      </div>
      
      <div class="content-box">
        <h3><i class="fas fa-heartbeat"></i> ヘルスケアと医療</h3>
        <p>医療分野でも深層強化学習の応用が進んでいます。特に個別化医療や治療計画の最適化において、<span class="key-term">オフライン強化学習</span>など、既存データから安全に学習する手法が重要です。</p>
        
        <div class="note-box">
          <p>医療分野では実験的な試行が難しいため、<span class="highlight">既存の医療データから学習する</span>オフライン強化学習が重要です。また、決定の<span class="highlight">説明可能性</span>も重視されるため、解釈可能なモデルの開発も進んでいます。</p>
        </div>
        
        <ul>
          <li>個別化された治療計画の最適化</li>
          <li>薬物投与量の調整</li>
          <li>臨床試験デザインの最適化</li>
          <li>医療資源の効率的な配分</li>
        </ul>
      </div>
      
      <div class="content-box">
        <h3><i class="fas fa-comment-dots"></i> 自然言語処理との融合</h3>
        <p>言語モデルの学習にも強化学習が活用されています。特に<span class="key-term">RLHF</span> (Reinforcement Learning from Human Feedback) は、人間のフィードバックを報酬として利用する手法で、ChatGPTなどの大規模言語モデルの性能向上に貢献しています。</p>
        
        <div class="handwritten-box">
          <h4>RLHF（人間のフィードバックからの強化学習）のプロセス</h4>
          <ol>
            <li>事前学習済み言語モデルの準備</li>
            <li>人間による回答の好みのラベル付け</li>
            <li>報酬モデルの学習</li>
            <li>強化学習による言語モデルの最適化</li>
          </ol>
        </div>
      </div>
      
      <div class="speech-bubble">
        <p>深層強化学習の実世界応用では、シミュレーションと現実のギャップ、サンプル効率、安全性の確保といった課題が依然として存在します。しかし、これらの課題を解決するための研究も急速に進んでおり、今後ますます幅広い分野での応用が期待されています。</p>
      </div>
    </section>

    <!-- コンテンツセクション - 主要アルゴリズム -->
    <section id="key-algorithms">
      <h2><i class="fas fa-microchip"></i> 主要アルゴリズム</h2>
      
      <p>深層強化学習には様々なアルゴリズムが存在し、それぞれに特徴と適用場面があります。ここでは、G検定のシラバスに記載されている主要アルゴリズムについて解説します。</p>
      
      <div class="content-box">
        <h3><i class="fas fa-code-branch"></i> 価値ベースの手法</h3>
        <p>価値ベースの手法は、状態や行動の価値を推定し、それに基づいて行動を選択します。深層Q学習（DQN）とその派生手法が代表的です。</p>
        
        <table>
          <tr>
            <th>アルゴリズム</th>
            <th>特徴</th>
            <th>長所・短所</th>
          </tr>
          <tr>
            <td><span class="key-term">DQN</span></td>
            <td>Q学習にディープニューラルネットワークを組み合わせた手法</td>
            <td>離散的な行動空間に適する。経験リプレイとターゲットネットワークで安定化</td>
          </tr>
          <tr>
            <td><span class="key-term">ダブルDQN</span></td>
            <td>行動選択と評価を別々のネットワークで行う</td>
            <td>Q値の過大評価を抑制し、より正確な価値推定が可能</td>
          </tr>
          <tr>
            <td><span class="key-term">デュエリングネットワーク</span></td>
            <td>状態価値と行動優位性を別々に推定</td>
            <td>状態の価値と各行動の相対的な優位性を分離して学習</td>
          </tr>
          <tr>
            <td><span class="key-term">Rainbow</span></td>
            <td>複数のDQN改良手法を統合したアルゴリズム</td>
            <td>様々な改良を組み合わせて性能向上。実装は複雑</td>
          </tr>
        </table>
        
        <div class="note-box">
          <p>価値ベースの手法は比較的実装が容易で理解しやすいのが特徴です。特に<span class="highlight">離散的な行動空間</span>を持つ問題に適しています。しかし、連続的な行動空間や高次元の問題では効率が低下する場合があります。</p>
        </div>
      </div>
      
      <div class="content-box">
        <h3><i class="fas fa-random"></i> 方策ベースの手法</h3>
        <p>方策ベースの手法は、状態から行動への写像（方策）を直接学習します。連続的な行動空間を持つ問題に適しています。</p>
        
        <div class="handwritten-box">
          <h4>主な方策ベースアルゴリズム</h4>
          <ul>
            <li><span class="key-term">REINFORCE</span>：基本的な方策勾配法。サンプル効率は低いが実装は簡単</li>
            <li><span class="key-term">A3C</span> (Asynchronous Advantage Actor-Critic)：並列学習による効率化</li>
            <li><span class="key-term">PPO</span> (Proximal Policy Optimization)：方策更新を制約して安定化</li>
          </ul>
        </div>
        
        <p>方策ベースの手法は、<span class="key-term">方策勾配法</span>を基礎としており、方策のパラメータを報酬の期待値が最大化される方向に更新します。特に<span class="key-term">PPO</span>は実装の容易さと性能のバランスが良く、広く使われています。</p>
      </div>
      
      <div class="content-box">
        <h3><i class="fas fa-balance-scale"></i> Actor-Critic法</h3>
        <p><span class="key-term">Actor-Critic</span>法は、価値ベースと方策ベースのアプローチを組み合わせた手法です。Actorは方策を学習し、Criticは状態価値を評価します。</p>
        
        <div class="note-box">
          <p>Actor-Critic法の特徴は<span class="highlight">サンプル効率と安定性のバランス</span>にあります。Criticによる価値推定が分散を低減し、より効率的な学習を可能にします。A3C、PPO、SACなど多くの最新アルゴリズムはActor-Critic構造を採用しています。</p>
        </div>
      </div>
      
      <div class="content-box">
        <h3><i class="fas fa-project-diagram"></i> 分散強化学習</h3>
        <p>複数のエージェントが並列に環境と相互作用し、経験を共有することで学習を効率化する手法です。</p>
        
        <ul>
          <li><span class="key-term">A3C</span>：非同期分散型のActor-Critic法</li>
          <li><span class="key-term">APE-X</span>：分散型の経験収集と優先度付き経験リプレイを組み合わせたアルゴリズム</li>
          <li><span class="key-term">Agent57</span>：様々な探索戦略を持つエージェントを組み合わせたアルゴリズム</li>
        </ul>
      </div>
      
      <div class="handwritten-box">
        <h3>その他の重要な手法</h3>
        <ul>
          <li><span class="key-term">報酬成形</span>：学習を容易にするための報酬設計手法</li>
          <li><span class="key-term">残差強化学習</span>：既存の方策を拡張するアプローチ</li>
          <li><span class="key-term">状態表現学習</span>：効率的な状態表現を学習する手法</li>
          <li><span class="key-term">オフライン強化学習</span>：事前に収集されたデータから学習する手法</li>
          <li><span class="key-term">マルチエージェント</span>：複数のエージェントが存在する環境での学習</li>
          <li><span class="key-term">連続値制御</span>：連続的な行動空間を持つ問題の制御</li>
        </ul>
      </div>
      
      <div class="speech-bubble">
        <p>深層強化学習の各アルゴリズムは、それぞれ異なる特性と適用範囲を持っています。実際の問題に取り組む際は、問題の性質（離散/連続行動空間、サンプル効率の重要性など）に応じて適切なアルゴリズムを選択することが重要です。</p>
      </div>
    </section>

    <!-- 用語集セクション -->
    <section id="glossary">
      <h2><i class="fas fa-book"></i> 用語集</h2>
      
      <div class="glossary">
        <table class="glossary-table">
          <tr>
            <th>用語</th>
            <th>説明</th>
          </tr>
          <tr>
            <td><span class="glossary-term">DQN</span><br><span class="glossary-en">Deep Q-Network</span></td>
            <td>Q学習とディープニューラルネットワークを組み合わせた強化学習アルゴリズム。Atariゲームで人間レベルのパフォーマンスを達成した。</td>
          </tr>
          <tr>
            <td><span class="glossary-term">PPO</span><br><span class="glossary-en">Proximal Policy Optimization</span></td>
            <td>方策勾配法の一種で、方策の更新幅を制限して学習の安定性を高めた手法。実装が容易で広く使われている。</td>
          </tr>
          <tr>
            <td><span class="glossary-term">A3C</span><br><span class="glossary-en">Asynchronous Advantage Actor-Critic</span></td>
            <td>複数のエージェントが並列に学習し、非同期的にパラメータを更新するActor-Critic法。</td>
          </tr>
          <tr>
            <td><span class="glossary-term">RLHF</span><br><span class="glossary-en">Reinforcement Learning from Human Feedback</span></td>
            <td>人間からのフィードバックを報酬として利用する強化学習手法。大規模言語モデルの学習に応用されている。</td>
          </tr>
          <tr>
            <td><span class="glossary-term">アルファ碁</span><br><span class="glossary-en">AlphaGo</span></td>
            <td>DeepMindが開発した囲碁AI。深層強化学習とモンテカルロ木探索を組み合わせ、人間のトッププロに勝利した。</td>
          </tr>
          <tr>
            <td><span class="glossary-term">アルファスター</span><br><span class="glossary-en">AlphaStar</span></td>
            <td>DeepMindが開発したスタークラフトIIプレイAI。マルチエージェント深層強化学習を用いてプロレベルの戦略を習得した。</td>
          </tr>
          <tr>
            <td><span class="glossary-term">Rainbow</span></td>
            <td>DQNの複数の改良手法を統合した強化学習アルゴリズム。優先度付き経験リプレイ、ダブルDQN、デュエリングネットワークなどを組み合わせている。</td>
          </tr>
          <tr>
            <td><span class="glossary-term">APE-X</span></td>
            <td>分散型の経験収集と優先度付き経験リプレイを組み合わせた強化学習アルゴリズム。並列処理によりサンプル効率を向上させる。</td>
          </tr>
          <tr>
            <td><span class="glossary-term">Agent57</span></td>
            <td>DeepMindが開発した強化学習エージェント。複数の探索戦略を組み合わせた適応型アルゴリズムで、全57種類のAtariゲームで人間を上回る性能を達成した。</td>
          </tr>
          <tr>
            <td><span class="glossary-term">sim2real</span></td>
            <td>シミュレーション環境で学習したモデルを実世界に転移する技術。ロボティクスなど実世界応用での重要な課題。</td>
          </tr>
          <tr>
            <td><span class="glossary-term">ドメインランダマイゼーション</span><br><span class="glossary-en">Domain Randomization</span></td>
            <td>シミュレーション環境のパラメータをランダムに変化させることで、実世界の多様性に対応できるモデルを学習する手法。</td>
          </tr>
          <tr>
            <td><span class="glossary-term">OpenAI Five</span></td>
            <td>OpenAIが開発したDota 2ゲーム用のAIシステム。チームベースのゲームで人間のプロチームに勝利した。</td>
          </tr>
        </table>
      </div>
    </section>

    <!-- Key Insights セクション -->
    <section id="key-insights" class="key-insights">
      <h2><i class="fas fa-lightbulb"></i> Key Insights</h2>
      
      <ul class="insights-list">
        <li>
          <h3>深層強化学習の基本概念</h3>
          <p>深層強化学習は従来の強化学習にディープニューラルネットワークを組み合わせたもので、状態表現、方策関数、価値関数などをニューラルネットワークで表現します。これにより、複雑な問題や生の入力データ（画像など）から直接学習することが可能になりました。</p>
        </li>
        <li>
          <h3>代表的なモデルと応用</h3>
          <p>DQN、PPO、A3C、Rainbow、AlphaGoなどの代表的なモデルが開発され、ゲームからロボティクス、自然言語処理まで広範囲に応用されています。特にRLHFは大規模言語モデルの性能向上に貢献しています。</p>
        </li>
        <li>
          <h3>深層強化学習の課題と発展方向</h3>
          <p>サンプル効率、安定性、一般化能力などに課題がありますが、オフライン強化学習、分散学習、マルチタスク学習などの手法で改善が進んでいます。実世界応用ではsim2realやドメインランダマイゼーションなどの技術が重要です。</p>
        </li>
      </ul>
    </section>

    <!-- Take Home Message セクション -->
    <section id="take-home" class="take-home">
      <h2><i class="fas fa-key"></i> Take Home Message</h2>
      
      <div class="take-home-message">
        深層強化学習は「環境との相互作用による学習」と「ディープラーニングの表現力」を組み合わせることで、複雑な問題を解決する強力なアプローチです。DQN、PPO、A3C、RLHF、アルファ碁などの重要なアルゴリズムと応用例を理解しましょう。
      </div>
    </section>

    <!-- フッター -->
    <footer>
      <p>G検定学習ノート：1-6-4 深層強化学習</p>
      <p>作成：<span id="current-date"></span> | テキスト：Claude | 画像：ChatGPT</p>
    </footer>

    <!-- スクロールトップボタン -->
    <div class="scroll-top">
      <i class="fas fa-arrow-up"></i>
    </div>
  </main>

  <script>
    // 現在の日付を設定
    const now = new Date();
    const options = { year: 'numeric', month: 'long', day: 'numeric' };
    document.getElementById('current-date').textContent = now.toLocaleDateString('ja-JP', options);
    
    // スクロールトップボタンの表示/非表示
    const scrollTopBtn = document.querySelector('.scroll-top');
    window.addEventListener('scroll', () => {
      if (window.pageYOffset > 300) {
        scrollTopBtn.classList.add('visible');
      } else {
        scrollTopBtn.classList.remove('visible');
      }
    });
    
    // スクロールトップボタンのクリックイベント
    scrollTopBtn.addEventListener('click', () => {
      window.scrollTo({
        top: 0,
        behavior: 'smooth'
      });
    });
    
    // サイドバーのリンクをクリックしたときのスムーススクロール
    document.querySelectorAll('.nav-links a').forEach(link => {
      link.addEventListener('click', function(e) {
        e.preventDefault();
        const targetId = this.getAttribute('href');
        const targetSection = document.querySelector(targetId);
        
        window.scrollTo({
          top: targetSection.offsetTop - 20,
          behavior: 'smooth'
        });
        
        // アクティブリンクの更新
        document.querySelectorAll('.nav-links a').forEach(link => {
          link.classList.remove('active');
        });
        this.classList.add('active');
      });
    });
    
    // スクロール位置に応じてナビゲーションの現在地を更新
    const sections = document.querySelectorAll('section');
    window.addEventListener('scroll', () => {
      let current = '';
      
      sections.forEach(section => {
        const sectionTop = section.offsetTop;
        const sectionHeight = section.clientHeight;
        if (pageYOffset >= (sectionTop - 100)) {
          current = section.getAttribute('id');
        }
      });
      
      document.querySelectorAll('.nav-links a').forEach(link => {
        link.classList.remove('active');
        if (link.getAttribute('href') === `#${current}`) {
          link.classList.add('active');
        }
      });
    });
    
    // レスポンシブメニュートグル（追加UI要素）
    if (window.innerWidth <= 992) {
      const menuToggle = document.createElement('div');
      menuToggle.className = 'menu-toggle';
      menuToggle.innerHTML = '<i class="fas fa-bars"></i>';
      document.body.appendChild(menuToggle);
      
      menuToggle.addEventListener('click', () => {
        document.querySelector('.sidebar').classList.toggle('active');
        menuToggle.innerHTML = document.querySelector('.sidebar').classList.contains('active') 
          ? '<i class="fas fa-times"></i>' 
          : '<i class="fas fa-bars"></i>';
      });
    }
  </script>
</body>
</html> 