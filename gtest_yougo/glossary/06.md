# 第6章 ディープラーニングの応用例 用語集

| 用語（日本語） | 用語英語（略語） | 解説 | POINT |
|---|---|---|---|
| 画像分類 | image classification | 入力画像に対し1つ（または複数）のラベルを割り当てるタスク。CNN/ViTを用い、損失は多くがsoftmax+交差エントロピー。データ拡張・正則化・転移学習で汎化を高める。 | データセットのラベルノイズやクラス不均衡（重み付け、再サンプリング）に注意。 |
| 物体検出 | object detection | バウンディングボックスとクラスを同時に推定。2段（Faster R-CNN系）と1段（YOLO/SSD系）に大別。IoUで一致度を測り、NMSで重複を抑制。 | 精度-速度-計算資源のトレードオフ。小物体・高密度シーンへの工夫（FPN、アンカーデザイン）。 |
| セマンティックセグメンテーション | semantic segmentation | 各画素をクラス分類するピクセル単位の識別。FCN/UNet/DeepLabが代表。ダイレーション畳み込みやASPPで広域文脈を取り込む。 | クラス境界のぼけ対策（境界損失、精細なスキップ）。評価はmIoU。 |
| インスタンスセグメンテーション | instance segmentation | 同一クラス内の個体ごとにマスクを分離。Mask R-CNN等が代表。検出＋マスク推定の2系統を統合。 | 重なりの多いシーンや細長い物体が難所。 |
| Vision Transformer | Vision Transformer (ViT) | 画像をパッチに分割しトークン化、Transformerで自己注意により長距離依存を学習。大規模事前学習と微調整で高精度。 | 画像固有の帰納バイアスが弱いためデータ拡張・正規化・大規模事前学習が鍵。 |
| 自然言語の特徴表現 | text representations | BoW/TF-IDFなどの疎ベクトルから、word2vec/fastText/GloVeの密な埋め込みへ発展。文脈依存表現はELMo/BERTなど深層モデルで獲得。 | 前処理（分かち書き、サブワード化）で性能が大きく変わる。 |
| BERT | BERT (bidirectional encoder representations from transformers) | 双方向Transformerエンコーダを用いた事前学習モデル。MLM（マスク化言語モデル）とNSPで事前学習し、下流タスクに微調整。 | 位置付けはエンコーダ系。長文外挿や速度に課題→Distil/Longformer等の派生。 |
| GPT | GPT (generative pre-trained transformer) | 自回帰的言語モデル。次トークン予測で事前学習し、生成や要約・翻訳などに適用。指示追従はSFTやRLHFで強化。 | 推論は左から右。コンテキスト長・デコーディング戦略（温度/トップk/p）が出力品質に影響。 |
| Seq2Seq + Attention | sequence-to-sequence with attention | エンコーダで系列を表現し、デコーダが注意で関連部分を参照して出力系列を生成。翻訳・要約・対話などに適用。 | TransformerはRNN+Attentionの並列化・長距離依存を強化した発展形。 |
| 音声特徴量（MFCC等） | MFCC and spectral features | 音声を短時間フレームに分割し、FFT→メル尺度→ケプストラムでMFCCを得る。スペクトログラム/ログメルも広く利用。 | マイク・環境ノイズや話者差を考慮（正規化/増強）。 |
| HMM | hidden Markov model (HMM) | 音声認識の古典的基盤。隠れ状態の遷移と出力確率で系列を表現し、Viterbiで最尤経路を推定。近年はDNNと結合（DNN-HMM）やEnd-to-Endへ。 | CTC/Attentionベースが普及しても、辞書や言語モデルとの統合で依然有用。 |
| WaveNet | WaveNet | 自回帰畳み込みで高品質な音声波形を生成するモデル。拡張として並列生成や条件付け（話者/テキスト）がある。 | 生成速度が課題→並列化・ボコーダの工夫（WaveGlow、HiFi-GANなど）。 |
| 強化学習（定式化） | reinforcement learning (formulation) | MDPで状態$s$・行動$a$・報酬$r$・遷移$P$・割引$\gamma$を定義。価値関数$V^\pi, Q^\pi$と最適方策$\pi^*$を求める。 | ベルマン方程式が中心。探索と活用のバランス（$\epsilon$-greedy、UCB等）。 |
| Q学習/DQN | Q-learning / deep Q-network (DQN) | オフポリシーで$Q$を更新。$Q\leftarrow Q+\alpha\big(r+\gamma\max_{a'}Q(s',a')-Q(s,a)\big)$。DQNはターゲットネット/経験再生で安定化。 | 過推定対策（Double DQN）、分布的RL、デュエリング、NoisyNetなど改良群。 |
| PPO | Proximal Policy Optimization (PPO) | 方策勾配のクリッピング目的で安定性とサンプル効率を両立。目的$\mathbb{E}[\min(r_t(\theta)A_t, \mathrm{clip}(r_t,1-\epsilon,1+\epsilon)A_t)]$。 | クリップ幅$\epsilon$と価値損失/エントロピー係数のバランス調整が重要。 |
| GAN | generative adversarial network (GAN) | 生成器$G$と識別器$D$のミニマックス最適化。$\min_G\max_D \mathbb{E}_{x\sim p_{data}}[\log D(x)] + \mathbb{E}_{z\sim p(z)}[\log(1-D(G(z)))]$。高品質生成だが不安定になりやすい。 | 損失（WGAN-GP等）や正則化、アーキ設計（SN、ResBlock）で安定化。 |
| 拡散モデル | diffusion model | ノイズ付加過程と逆過程を学習し、段階的にノイズを除去して生成。スコアマッチング/DDPM/スケジュール設計が鍵。高Fidelity・多様性でSOTA。 | 速度最適化（少ステップ化）やガイダンス（Classifier/CFG）で精度制御。 |
| NeRF | neural radiance fields (NeRF) | 位置と視線方向を入力して放射輝度と密度を出力するMLPを体積レンダリングで学習。新規視点合成で高品質。 | サンプル効率と速度が課題→ハッシュグリッド/分割表現/蒸留で高速化。 |
| 転移学習 | transfer learning | 事前学習モデルの表現を下流タスクへ流用。特徴抽出（凍結）から全層微調整まで段階がある。データ不足で特に有効。 | 学習率や凍結解除の順序、正則化（重み減衰/早期終了）で過適合を防ぐ。 |
| ファインチューニング | fine-tuning | 事前学習済み重みを初期値として下流タスクで再学習。学習率を小さく、ヘッドから段階的に凍結解除する手法が一般的。 | 破壊的忘却に注意。小さめ学習率・データ拡張・正則化で緩和。 |
| 自己教師あり学習 | self-supervised learning | ラベル不要の事前学習。マスク予測（MAE/MLM）、対比学習（SimCLR/CLIP）、自己回帰などで表現を獲得。 | タスク設計が性能を左右。下流一致（pretextと用途のギャップ）に留意。 |
| CLIP | CLIP | 画像エンコーダとテキストエンコーダを対比学習で共同学習。ゼロショット分類や検索が可能。 | プロンプト設計やテキスト表現が性能に影響。 |
| Text-to-Image | text-to-image generation | テキスト条件で画像を生成。拡散モデルが主流（GLIDE/Stable Diffusion等）。 | 安全性・著作権・バイアスの配慮が必要。 |
| 説明可能性（Grad-CAM等） | explainability (Grad-CAM, LIME, SHAP) | 出力に対する入力寄与を可視化・推定する手法群。Grad-CAMはクラス勾配×特徴マップの重みでヒートマップを得る。LIME/SHAPは近傍線形/ゲーム理論で寄与を推定。 | 手法ごとの前提・解像度・安定性が異なる。評価は注意。 |
| 軽量化（蒸留） | knowledge distillation | 大モデル（教師）の振る舞いを小モデル（生徒）へ移植。温度付きsoftmaxや中間表現の蒸留で精度維持を図る。 | 教師の精度と蒸留損失設計がカギ。 |
| 量子化 | quantization | 浮動小数を低精度整数へ写像（INT8等）。後量子化（PTQ）と量子化対応学習（QAT）がある。 | スケール/ゼロ点の校正、演算子対応、精度低下のバランス。 |
| プルーニング | pruning | 重要度の低い重みやチャネルを削除し疎構造化。構造化/非構造化の手法がある。 | スパース性のハードウェア実効、再学習での回復を考慮。 |
| 宝くじ仮説 | lottery ticket hypothesis | 適切に初期化されたサブネット（当たりくじ）があり、それを抽出・再学習すると高精度を保てるという仮説。 | プルーニングと関連。初期化・再初期化の扱いに注意。 |
| ResNet | residual network (ResNet) | 残差ブロック$y=F(x)+x$で勾配経路を確保し、深層化を可能にしたCNN。ボトルネックとPre-Activationで学習安定。 | 同じ次元なら恒等スキップ、次元増は$1\times1$で整合。 |
| DenseNet | densely connected network | 各層の出力を後続層へ連結（concat）し特徴再利用と勾配流を強化。パラメータ効率に優れる。 | 成長率や圧縮率の設計が重要。メモリ使用量に注意。 |
| EfficientNet | EfficientNet | 複合スケーリングで深さ/幅/解像度を共同に拡大。MBConvやSEで効率と精度を両立。 | φ（compound coefficient）の設定で計算-精度を調整。 |
| Faster R-CNN | Faster R-CNN | RPNで候補領域を生成しRoIヘッドで分類/回帰する2段検出。高精度だが速度は1段系に劣る。 | アンカー設計と特徴ピラミッド（FPN）の有無が精度を左右。 |
| YOLO | YOLO family | 1段検出。単一ネットでグリッド上にボックス/クラスを同時回帰。リアルタイム性に優れる。 | 近年はアンカーフリーや拡張損失（CIoU等）の採用が一般的。 |
| U-Net | U-Net | U字型のEncoder-Decoderにスキップで高解像の特徴を結合。医用などのセグメンテーションで強力。 | デコーダのアップサンプリングとスキップの整合が精度に影響。 |
| DeepLab | DeepLab | ASPP（空間ピラミッド）とダイレーション畳み込みで広域文脈を取り込むセグメンテーション系。 | 出力ストライドやダイレーション設定が鍵。 |
| Transformer | Transformer | 自己注意と位置エンコーディングで系列の長距離依存を並列に学習。エンコーダ/デコーダ構成。 | 学習安定には正規化/ドロップアウト/学習率スケジュール（Warmup）が重要。 |
| CBOW/Skip-gram | CBOW / skip-gram (word2vec) | 近傍から中心語（CBOW）/中心語から近傍（Skip-gram）を予測し埋め込みを学習。負例サンプリングで高速化。 | サブワード化（fastText）で未登録語に頑健。 |
| TF-IDF | TF-IDF | 用語の出現頻度（TF）と文書頻度の逆数（IDF）で重み付けした疎ベクトル表現。 | ストップワード除去や正規化で性能改善。 |
| CTC | connectionist temporal classification (CTC) | 入出力長が異なる系列をアライメント不要で学習。空白トークンと反復の圧縮で出力系列を定義。 | ビームサーチや言語モデル併用で精度向上。 |
| SpecAugment | SpecAugment | スペクトログラムに時間/周波数のマスキングやワーピングを施す音声データ拡張。 | 過度なマスクで情報損失に注意。タスクに合わせて強度調整。 |
| A3C/Actor-Critic | A3C / actor-critic | 価値推定器と方策を同時学習。A3Cは分散並列でサンプルを収集し安定化/高速化。 | 価値関数のリークや高分散に注意。Advantage推定（GAE）で安定化。 |
| REINFORCE | REINFORCE | 方策勾配の基礎。$\nabla_\theta J=\mathbb{E}[\nabla_\theta\log \pi_\theta(a\mid s)\,R]$。分散が大きいためベースラインで低減。 | エピソード型報酬で遅延報酬に弱い→Actor-Criticへ発展。 |
| RLHF | reinforcement learning from human feedback | 生成モデルに人間の好みを反映させる枠組み。報酬モデル学習→方策最適化（PPO等）。 | 報酬ハッキング/整合性/安全性に留意。データのバイアス管理が重要。 |
| DCGAN | DCGAN | 画像生成に特化したConvベースのGAN。BNとReLU/LeakyReLUで安定化。 | 初期設計のベースラインとして有用。 |
| Pix2Pix | Pix2Pix | 条件付きGANでペアデータの画像変換（例えば輪郭→写真）。$L1$再構成損失を併用。 | ペアデータが必要。CycleGANは非ペア対応。 |
| CycleGAN | CycleGAN | サイクル一貫性損失で非ペア画像間の変換を学習（A→B→A復元）。 | モード崩壊や色ずれ対策に識別器/損失の設計が影響。 |
| Few-shot/One-shot | few-shot / one-shot learning | 少量サンプルで学習/適応する枠組み。メタ学習（MAML、ProtoNet）や事前学習表現の活用で実現。 | エピソディック学習・タスク分布の設計が鍵。 |
| VQA | visual question answering (VQA) | 画像と自然言語の質問から回答を生成/分類。注意機構とマルチモーダル融合（加算/積/共注意）を用いる。 | 言語バイアス/データリークに注意。 |
| DALL-E | DALL-E | テキストから画像を生成するモデル群。離散潜在（VQ）や拡散を活用し大規模データで学習。 | 安全性や権利配慮、プロンプト設計が性能に影響。 |
| 大規模言語モデル | large language model (LLM) | 主にTransformerデコーダを用い、大規模コーパスで次トークン予測の事前学習を行う生成モデル。スケーリング則に従いパラメータ数とデータ/計算を増やすと損失が滑らかに低下し能力が向上。事前学習後に整合や安全性のためのアラインメントが行われる。 | 記憶ではなく分布学習。出力の確率性とデータ依存のバイアスを理解。用途ではガードレールと評価を設計。 |
| インストラクションチューニング | instruction tuning (SFT) | 指示と応答のペアに対する教師あり微調整で、指示追従性とフォーマット整合を向上。一般には事前学習モデルの上に数エポックで実施。 | データ品質が支配的。汎化を高める多様なタスクとテンプレート設計が重要。 |
| 直接嗜好最適化 | direct preference optimization (DPO) | 人間の好み比較データから方策を直接最適化する整合手法。報酬モデルを明示的に学習せず、勝ちペアの尤度を高め負けを下げる目的で最適化。 | RLHFより実装が簡潔で安定しやすいが、データのバイアスに注意。 |
| RAG | retrieval-augmented generation (RAG) | 検索器で関連文書を取得し、プロンプトに組み込んで生成する枠組み。埋め込みとベクターストア、チャンク設計、再ランキングの質が鍵。知識の鮮度と根拠提示が得られる。 | 参照の品質管理とプロンプトインジェクション対策が必須。キャッシュや再検索でレイテンシと精度を両立。 |
| LoRA | low-rank adaptation (LoRA) | 低ランク行列で重み更新を近似し、微調整時の学習パラメータとメモリを大幅削減。元重みは凍結し追加行列のみ学習。 | マージや合成が容易。タスク依存でランクや適用層を調整。 |
| QLoRA | QLoRA | 4ビット量子化でベースモデルを保持しつつLoRAで微調整する省メモリ手法。ダブル量子化やページング最適化でVRAMを節約。 | 量子化誤差と精度のトレードオフ。スケール選択と勾配安定に留意。 |
| PEFT | parameter-efficient fine-tuning (PEFT) | LoRAやPrefix-tuning、Adapter、Prompt-tuningなど、パラメータの一部のみ更新する微調整群の総称。 | 資源制約や多数タスクの同居に有効。ヘッドのみ学習から段階的解凍まで戦略を設計。 |
| プロンプトエンジニアリング | prompt engineering | Few-shotや思考の連鎖、自己一貫性、システムプロンプト設計などで振る舞いを制御する実務手法。フォーマット規約や出力制約で後段処理を容易にする。 | 長すぎるプロンプトはコストと逸脱を招く。検証用のテンプレとテストケースを用意。 |
| ツール使用と関数呼び出し | tool use and function calling | モデルが外部APIや関数仕様に従って構造化呼び出しを行い、結果を統合して応答を生成する枠組み。計算器やデータベース、検索、コード実行などのツール連携。 | スキーマ定義とバリデーション、失敗時のリトライ戦略、監査ログが重要。 |
| マルチモーダルLLM | multimodal LLM (VLM) | 画像や音声など非テキスト入力を統合して推論するLLM。例としてFlamingo、LLaVA、GPT-4Vなど。視覚特徴を投影し言語表現と融合する。 | 入力前処理と安全性評価が難しい。画像中テキストや表の扱いで精度差が出る。 |
| ハルシネーション | hallucination | もっともらしいが事実と異なる内容を生成する現象。未知領域や曖昧なプロンプト、強い制約下で増える。 | RAGで根拠付け、温度/出力検証、ルーブリック評価で抑制。重要用途は人手確認を前提に。 |
| ガードレール | guardrails (safety/controls) | モデルの出力やツール呼び出しをポリシーで制御し、安全性・適法性・コンプライアンスを担保する実装群。具体例は出力の安全分類器、JSON/関数スキーマ検証、禁止トピック/PIIのマスキング、監査ログ、レート制御など。 | ルールは静的だけでなく運用で継続改善。誤検知/過剰抑制のバランスと例外ハンドリングを設計。 |
| プロンプトインジェクション | prompt injection | 外部テキスト（検索/RAG/ユーザ入力）に埋め込まれた指示でモデルの本来の制約や方針を上書きさせる攻撃。システム/開発者/ユーザの指示階層の混同や、ツール呼び出しの悪用が典型。 | 対策: コンテキスト分離、危険トークンの正規化/除去、ツールのホワイトリスト・スキーマ検証、出力フィルタ、RAGのソース検証と再ランキング。 |
| 評価指標（LLM） | evaluation for LLMs | 参照あり自動評価（BLEU/ROUGE/METEOR）、意味類似（BERTScore）、事実性/忠実度（QAGS/FactCC）、RAG向けのFaithfulness/Answer Relevance、審査員としてのLLM（LLM-as-a-judge）と人手評価を併用。タスク別にルーブリックを設け、再現可能なベンチ（MT-Bench等）で比較する。 | 自動評価は過信禁物。分布外や安全性は人手を含む多面的評価で担保。メトリクスは目的（正確性/網羅性/安全/スタイル）に合わせ選択。 |
