# 第5章 ディープラーニングの要素技術 用語集

| 用語（日本語） | 用語英語（略語） | 解説 | POINT |
|---|---|---|---|
| 全結合層 | fully connected layer (FC) | 入力ベクトル$\in\mathbb{R}^{C_{in}}$に対して$y=W x+b$（$W\in\mathbb{R}^{C_{out}\times C_{in}}$）を計算する基本層。表現力は高いがパラメータ数が多く過学習や計算コストが増大しやすい。画像ではFlattenで空間構造を捨てるためCNNより不利なことが多い。 | パラメータ数は$C_{in}\times C_{out}$（+バイアス）。正則化（L2/Dropout）や正規化層と併用。 |
| 畳み込み層 | convolutional layer (Conv) | カーネル（フィルタ）で局所領域を線形結合し、重みを空間共有する層。2Dなら出力チャネル数$C_{out}$、入力$C_{in}$、カーネル$k_h\times k_w$でパラメータ$k_h k_w C_{in} C_{out}$（+バイアス$C_{out}$）。ストライド/パディング/ダイレーションで受容野と解像度を制御。 | 平行移動に対する同変性を獲得、パラメータ効率が高い。設計はカーネル/ストライド/パディングが要。 |
| ダイレーション畳み込み | dilated (atrous) convolution | カーネル要素間に間隔$d$を設けて受容野を拡大する畳み込み。パラメータ数を増やさずに広域文脈を取り込める。セマンティックセグメンテーション等で有効。 | 破線状の受容野によるグリッドアーチファクトに注意。多段で$d$を変える設計が一般的。 |
| 深さ方向分離畳み込み | depthwise separable convolution | Depthwise（チャネル毎の$k\times k$畳み込み）+ Pointwise（$1\times1$）で通常の$k^2 C_{in} C_{out}$を$k^2 C_{in} + C_{in} C_{out}$に分解し大幅に削減。MobileNet等で採用。 | 速度/省メモリに有利だが表現力は低下し得る。ボトルネック設計やSE/残差補強と併用。 |
| 1×1 畳み込み | pointwise convolution (1x1) | チャネル方向の線形結合（特徴量再配分・圧縮/拡張）。空間形状は変えず$C_{in}\to C_{out}$の写像を実現。 | ボトルネック（ResNet, Inception）で計算量削減・非線形の挿入に有効。 |
| 正規化層 | normalization layer | 活性分布を整えて学習を安定化し、より高い学習率や深いネットを可能にする層の総称。代表はBatchNorm/LayerNorm/Instance/Group Norm。 | 目的は内部共変量シフトの緩和と正則化効果。位置と順序（活性化の前後）に注意。 |
| バッチ正規化 | batch normalization (BN) | ミニバッチで$\mu,\sigma^2$を推定し、$\hat{x}=(x-\mu)/\sqrt{\sigma^2+\varepsilon}$、学習可能$\gamma,\beta$で$y=\gamma\hat{x}+\beta$。学習を安定・高速化し正則化効果も持つ。推論時は移動平均統計を使用。 | バッチサイズ依存やRNNとの相性に注意。小バッチではGroup/LayerNorm検討。 |
| レイヤー正規化 | layer normalization (LN) | 特徴次元に沿って正規化（サンプルごと）。RNN/Transformerと相性が良い。 | バッチに依存しない。TransformerはPre-LNが学習安定なことが多い。 |
| インスタンス正規化 | instance normalization (IN) | サンプル・チャネルごとに空間次元で正規化。スタイルの除去に寄与し、スタイル変換などで有効。 | コンピュータビジョンの生成系で頻用。コンテンツ保持とのバランスに注意。 |
| グループ正規化 | group normalization (GN) | チャネルをGグループに分割し、それぞれで正規化。小バッチでも安定し、検出・セグメンテーションで有効。 | Gの選択が重要（例: 32）。BNの置換として安定。 |
| プーリング層 | pooling layer | 空間領域を集約して不変性・ロバスト性を獲得しつつ解像度を低下させる。最大値プーリングはエッジ等の強い反応を保持、平均値プーリングは平滑化。 | 設計次第で情報損失が大きい。近年はストライドConvで代替する手法も多い。 |
| グローバル平均プーリング | global average pooling (GAP) | 特徴マップをチャネル毎に空間平均して$1\times1$に集約。全結合層の代替としてパラメータを削減し過学習を抑える。 | 分類ヘッドの軽量化に有効。CAM/Grad-CAMとの相性が良い。 |
| スキップ結合 | skip connection (residual) | 入力$x$を出力に恒等写像で加える$y=F(x)+x$。勾配経路が短絡され、深いネットでも学習が容易。ResNetではボトルネックとBN/活性化配置（Pre-Activation）が鍵。 | 恒等マッピングが学習しやすく初期性能も安定。次元不一致は$1\times1$で整合。 |
| 回帰結合層 | recurrent connection layer | 時刻$t$の出力/状態が次時刻の入力に回帰する結合。基本形は$h_t=f(W_x x_t + W_h h_{t-1}+b)$。時間方向にBPTTで学習し、長期依存で勾配消失/爆発が生じやすい。 | Teacher Forcing/スケジュールドサンプリング、双方向RNN、正規化/残差、クリッピングで緩和。 |
| Attention（スケールド内積） | scaled dot-product attention | $\mathrm{Attn}(Q,K,V)=\mathrm{softmax}(QK^\top/\sqrt{d_k})V$。クエリとキーの類似度に基づき値を加重和。長距離依存を並列に捉えられ、RNNのボトルネックを解消。 | マルチヘッドで表現多様性を確保。位置は相対/絶対位置エンコーディングで付与。計算量$O(n^2)$に注意。 |
| マルチヘッド注意 | multi-head attention (MHA) | $h$個のヘッドで$Q,K,V$を分割し並列に注意→結合。各ヘッドが異なるサブ空間の関係を学習。Transformerの中核。 | スケーリング/初期化の不一致で不安定化し得る。Dropoutや正規化で安定化。 |
| 位置エンコーディング | positional encoding | 自然な位置情報を持たない注意機構に位置を注入。正弦波PEは$PE_{pos,2i}=\sin(pos/10000^{2i/d})$等、学習PEは表現力が高い。 | 相対位置（RoPE/ALiBi等）は長文外挿性が高い。タスクで選択。 |
| オートエンコーダ | autoencoder (AE) | エンコーダ$z=f(x)$とデコーダ$\hat{x}=g(z)$で入力の再構成を学習。ボトルネックで次元圧縮・特徴抽出・ノイズ除去（Denoising AE）・スパース性（Sparse AE）を実現。損失はMSEやBCE。 | 表現が崩壊しないよう容量や正則化（スパース/ノイズ）を調整。過学習に注意。 |
| 変分オートエンコーダ | variational autoencoder (VAE) | 近似事後$\,q_\phi(z\mid x)\,$で$\,\mathbb{E}_{q}[\log p_\theta(x\mid z)]-\mathrm{KL}\big(q_\phi(z\mid x)\Vert p(z)\big)$（ELBO）を最大化。再パラメータ化$\,z=\mu+\sigma\odot\varepsilon\,$で勾配を流す。連続潜在で生成・補間が容易。 | KL重みの調整（β-VAE）で解離度/再構成のトレードオフ。事前分布/ポスターリオルのミスマッチに注意。 |
| VQ-VAE | vector-quantized VAE (VQ-VAE) | 潜在をコードブックで離散化し量子化。損失は再構成＋コードブック/コミットメント。GANを併用せずに高品質生成を達成する例も。 | 離散表現で長期依存やテキスト/音声に有利。コードブック崩壊に注意。 |
| データ拡張 | data augmentation | 学習データにラベル保存変換を施し汎化を改善。画像（Flip/Rotate/Crop/ColorJitter/Cutout/CutMix/Mixup）、言語（置換/並び替え/パラフレーズ）、音声（TimeMask/SpecAugment）など。 | リーク（テスト情報の混入）やラベル崩れに注意。強度や確率はタスクに合わせ調整（RandAugment等）。 |
| テスト時拡張 | test-time augmentation (TTA) | 推論時に複数変換した入力の予測を平均/多数決して頑健性を上げる。 | 速度と精度のトレードオフ。分布外での悪化に注意。 |

| 正規化の配置 | normalization placement | Transformer等ではPre-LN（層の前）とPost-LN（層の後）があり、Pre-LNは勾配が安定し深層化に強い。CNNではConv→BN→活性化（あるいはPre-Act ResNet）など順序が学習安定に影響。 | 位置で学習安定/収束性が変わる。実装の一貫性を保つ。 |
| 残差ブロックの順序 | residual block ordering | Pre-ActivationではBN→活性化→Conv→…→加算とし、勾配経路を素通しに近づける設計が一般的。 | BatchNorm統計とDropoutの位置に注意。 |
| BNとDropoutの相性 | BN and dropout | BNはバッチ統計に基づくため、同一層直後のDropoutで分布が揺れやすい。CNNではDropoutを減らしL2/データ拡張で代替、全結合終盤で限定的に使う設計が多い。 | 小バッチではBNが不安定→GN/LNを検討。推論時統計（moving average）を確認。 |
