# 第4章 ディープラーニングの概説 用語集

| 用語（日本語） | 用語英語（略語） | 解説 | POINT |
|---|---|---|---|
| ニューラルネットワーク | artificial neural network (ANN) | 人工ニューロン（重み付き和＋活性化）を層状に結んだ関数近似モデル。重みとバイアスを学習して入力から出力への非線形写像を獲得する。ユニバーサル近似定理により十分な幅/深さと適切な活性化があれば任意の連続関数を近似できる（実用上はデータ・正則化・最適化が制約）。 | 統計的学習の一種。汎化はモデル容量、データ量・質、正則化、最適化（初期化・学習率）に強く依存する。 |
| ディープラーニング | deep learning | 多層（深い）ニューラルネットワークで表現学習と最終タスクを一体的に学ぶ手法群。特徴量設計を人手に頼らず、データから多段の抽象表現を自動獲得する（画像= CNN、系列= RNN/LSTM、広範= Transformer）。大規模データと計算資源、適切な正則化・最適化が成功の鍵。 | 「深さ」は手段で、表現学習（分布や構造の抽象化）が本質。前処理・設計の負担が減る一方でデータ品質への感度は高い。 |
| パーセプトロン | perceptron | 単層の線形分類器。入力の線形結合にしきい値（符号）を適用して2値判定する。学習は誤分類がある限り重みを更新するパーセプトロン学習則で収束（線形分離可能な場合）。XORのような非線形分離問題は単層では表現不能。 | XOR=排他的論理和。例えば(0,0),(1,1)がクラスA、(1,0),(0,1)がクラスBは一本の直線で分けられない→多層化や非線形写像が必要。 |
| 多層パーセプトロン | multilayer perceptron (MLP) | 全結合（密結合）層を複数重ね、活性化で非線形性を導入する基本構造。中間層が基底関数の自動学習を担い、十分な幅/深さで任意の連続関数を近似可能。正規化（Batch/LayerNorm）や残差接続の導入で学習安定性が向上。 | 入力の局所性や平行移動不変を活かせないため、画像・音声ではCNN等が有利なことが多い。 |
| 畳み込みニューラルネットワーク | convolutional neural network (CNN) | カーネルとの畳み込みで局所受容野とパラメータ共有を実現し、平行移動に対する同変性（特徴マップのシフト）を獲得。プーリングやストライドで空間解像度を下げつつ不変性を強める。画像・動画・音声の空間的/時空間的構造に適合。 | 共有によりパラメータ効率・計算効率が高い。カーネルサイズ/ストライド/パディング設計が性能を左右。 |
| 再帰型ニューラルネットワーク | recurrent neural network (RNN) | 系列入力に対し、時刻$t$の隠れ状態$h_t=f(h_{t-1}, x_t)$で文脈を再帰的に保持するモデル。時間展開すると重み共有の深いネットとなり、長期依存で勾配消失/爆発が起きやすい。言語・音声・時系列予測などで利用。 | 勾配問題への対処としてLSTM/GRU、正規化（LayerNorm）、勾配クリッピング、残差接続、注意機構などがある。 |
| LSTM | long short-term memory (LSTM) | 入力・忘却・出力ゲートによりセル状態$c_t$と隠れ状態$h_t$の情報流を制御し、長期依存を扱いやすくしたRNN拡張。ゲートの活性化にσ、出力整形にtanhを用いる構成が一般的。 | パラメータはGRUより多いが表現力が高い場面もある。勾配消失を緩和し、長文や長期依存に強い。 |
| GRU | gated recurrent unit (GRU) | 更新・リセットの2ゲートで状態更新を制御する簡潔なRNN。LSTMに比べてパラメータが少なく、学習・推論が軽い割に精度は近いことが多い。 | 小〜中規模データや計算資源制約下で有利なことがある。LSTM/GRUはデータ特性で使い分け。 |
| CPU | central processing unit (CPU) | 汎用計算プロセッサ。分岐やメモリアクセスの不規則性に強く、前処理・データローディング・小規模バッチ推論・制御ロジックなどに適する。SIMDやベクトル命令である程度の並列も可能。 | 大規模行列演算のスループットはGPU/TPUが有利。CPUはレイテンシ重視の小規模推論で活躍。 |
| GPU | graphics processing unit (GPU) | 膨大なスレッドでSIMT並列を行うプロセッサ。行列・畳み込みなどデータ並列演算のスループットが高く、学習で広く利用される。主要APIはNVIDIAのCUDA、AMDのROCm。 | 性能はメモリ帯域とVRAM容量、バッチサイズに影響される。混合精度（FP16/bfloat16）で計算・メモリ効率を改善。 |
| TPU | tensor processing unit (TPU) | Googleが設計した行列演算特化のASICアクセラレータ。シストリックアレイ（行列乗算ユニット）による高スループット、XLAコンパイラ最適化、bfloat16/INT8対応などで大規模学習・推論に適する。主にGoogle CloudのTPU Podで利用。 | フレームワークはJAX/TFとの親和性が高い（XLA）。クラウド前提・割当/料金・ホストI/O制約を理解して選択。 |
| 活性化関数 | activation function | ニューロン出力に非線形性を導入する関数。飽和型（sigmoid/tanh）と非飽和型（ReLU系）で学習挙動が異なる。勾配の通りやすさ、ゼロ中心性、数値安定性に影響する。 | 役割は非線形化と表現力の確保。中間層は非飽和型が主流、出力層はタスクに応じて選択（2値=σ、多クラス=softmaxなど）。 |
| ReLU | rectified linear unit (ReLU) | $f(x)=\max(0,x)$。非飽和領域で勾配が保たれ、学習が速い。導関数は$x>0$で1、$x\le 0$で0。 | Dead ReLU（恒ゼロ化）に注意。He初期化、適切な学習率、Leaky/ELU/GELUの採用で緩和。 |
| Leaky ReLU | leaky rectified linear unit | $f(x)=\max(\alpha x, x)$（$0<\alpha\ll1$）。負側に小傾きを持たせてDead ReLUを緩和。 | $\alpha$は固定か学習可（PReLU）。負側の分布や勾配の流れを保ちたい場合に有効。 |
| シグモイド関数 | sigmoid | $\sigma(x)=\tfrac{1}{1+e^{-x}}$。出力が(0,1)で確率解釈が容易。導関数は$\sigma(x)(1-\sigma(x))$で、飽和域では勾配が小さい。 | 中間層では勾配消失を招きやすい。主に出力層（二値分類）やゲートの内部（LSTM等の0–1ゲート）で使用。 |
| tanh | hyperbolic tangent (tanh) | $tanh(x)=\tfrac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$。ゼロ中心でσより学習が進みやすい。導関数は$1-\tanh^2(x)$。 | 依然として飽和はあるため、初期化や正規化（Batch/LayerNorm）と併用して安定化。 |
| ソフトマックス関数 | softmax | 多クラスの確率に正規化。$softmax_i(z)=\tfrac{e^{z_i}}{\sum_j e^{z_j}}$。クロスエントロピーと組み合わせると勾配が$\,p_i-y_i\,$となり実装が簡潔。 | 数値安定化に$z\leftarrow z-\max(z)$（log-sum-exp）。温度付き$softmax(z/T)$で分布の鋭さを調整。 |

| 重み初期化 | weight initialization | 勾配の爆発/消失を防ぐため、活性化と層の入出力次元に応じた分散で初期化する。Xavier/Glorotは線形/双曲線型、He/KaimingはReLU系に適し、$\mathrm{Var}(W)=\tfrac{2}{fan\_in}$ などを用いる。 | 活性化/正規化との相性に注意。残差接続ではスケール過大にしない。 |
| 勾配クリッピング | gradient clipping | 勾配のノルムを閾値で制限し、勾配爆発を防ぐ。特にRNN/BPTTや大規模モデルで有効。 | クリップ基準（グローバルノルム/要素別）と閾値設定を明示。過度なクリップは学習を遅らせる。 |
| 学習率スケジュール | learning rate schedule | 収束と汎化のため学習率を時間変化させる。ステップ/指数/コサイン、Warmup→Cosine、One-cycleなどが一般的。プレートー検知で減衰する方法もある。 | Adam系でもベース学習率・Warmup設定が重要。バッチサイズスケール則を活用。 |
| 平均二乗誤差 | mean squared error (MSE) | 回帰で用いる基本損失。$\mathrm{MSE}=\tfrac{1}{n}\sum_i (y_i-\hat{y}_i)^2$。微分は$\partial \mathrm{MSE}/\partial \hat{y}_i=\tfrac{2}{n}(\hat{y}_i-y_i)$で実装が容易。二乗により大きな誤差を強く罰し、外れ値に敏感。 | スケールに依存するため特徴の標準化と併用される。外れ値が多いときはMAEやHuberも検討。 |
| 交差エントロピー | cross entropy | 分類で用いる基本損失。二値は$-\,[y\log p+(1-y)\log(1-p)]$、多クラスは$-\sum_i y_i\log p_i$。softmaxと組み合わせると勾配が$\,p_i-y_i\,$となり安定かつ効率的。 | ラベルスムージングで過信を抑制。数値安定のためlog-softmaxの同時計算を用いる実装が多い。 |
| KLダイバージェンス | Kullback–Leibler divergence (KL) | 分布$P$から$Q$への情報量差。$\mathrm{KL}(P\|Q)=\sum_i P(i)\log\tfrac{P(i)}{Q(i)}$（非対称）。分布整合（蒸留、VAEの正則化）で用いる。 | 距離ではなく非対称。Forward/Reverseで性質が異なる（モードカバリングvsモードシーキング）。 |
| コントラスト損失 | contrastive loss | ペア$(x_1,x_2)$の距離学習。$d=\|f(x_1)-f(x_2)\|$、同類$y=1$で$y\,d^2$、異類$y=0$で$\max(0,m-d)^2$。類似は近く、非類似はマージン$m$以上離す。 | ハードネガティブ採掘が精度に影響。埋め込みの正規化や適切な$m$選択が重要。 |
| トリプレット損失 | triplet loss | アンカー$a$・ポジティブ$p$・ネガティブ$n$で$\max(0,\,d(a,p)-d(a,n)+\alpha)$。ポジティブを近く、ネガティブをマージン$\alpha$だけ遠ざける。 | サンプリング戦略（ハード/セミハード）が学習を左右。距離の選択（L2/コサイン）も影響。 |
| 正則化 | regularization | 損失に罰則項を加えて過学習を抑制し汎化を改善する枠組み。$\mathcal{L}'=\mathcal{L}+\lambda\,\Omega(w)$。容量（自由度）や重みノルムを抑えることでバリアンスを減らし、わずかなバイアスと引き換えに汎化を高める。 | 目的は「汎化」。データ拡張・早期終了・重み減衰・ドロップアウト・正規化層なども広義の正則化として機能。 |
| L1 正則化（ラッソ） | L1 regularization (Lasso) | $\lambda\sum_j |w_j|$を付加。スパース解を誘発し、特徴選択の効果がある。外れ値に比較的頑健。 | 係数が厳密に0になりうる。凸だが非滑らかで最適化に注意（座標降下など）。 |
| L2 正則化（リッジ） | L2 regularization (Ridge) | $\tfrac{\lambda}{2}\sum_j w_j^2$を付加。大きな重みを滑らかに抑制し、数値安定性を高める。 | SGD系では「重み減衰」とほぼ同等（AdamはAdamWで減衰を分離）。過学習抑制の第一候補。 |
| ドロップアウト | dropout | 学習時にユニットを確率$p$で無効化し、逆に残りをスケーリングして学習する近似アンサンブル。推論時はスケーリングのみで全ユニット使用。 | BatchNormとの相性や位置（中間/終盤）に注意。過学習が強いときに有効。 |
| 誤差逆伝播法 | backpropagation | 連鎖律により勾配を層ごとに効率計算する手法。全結合層なら$\delta^{(l)}=(W^{(l+1)})^\top\delta^{(l+1)}\odot f'(z^{(l)})$、$\partial\mathcal{L}/\partial W^{(l)}=\delta^{(l)} (a^{(l-1)})^\top$。RNNでは時間方向に展開したBPTTで勾配を伝搬。 | 勾配消失/爆発の主因は飽和活性・長鎖乗算・不適切初期化。He/Xavier初期化、正規化、残差、クリッピングで緩和。 |
| 勾配降下法 | gradient descent | 全データの平均勾配で更新。$w\leftarrow w-\eta\,\nabla\mathcal{L}(w)$。収束は安定だが1ステップが重い。 | 大規模データでは非現実的。ミニバッチSGDが実務標準。 |
| 確率的勾配降下法 | stochastic gradient descent (SGD) | サンプルまたはミニバッチ勾配で更新。ノイズにより鞍点・浅い局所解からの脱出性が高まる。 | 典型はミニバッチSGD。シャッフル、学習率スケジュールで収束を整える。 |
| ミニバッチ学習 | mini-batch learning | バッチサイズ$B$で$\tfrac{1}{B}\sum_{i=1}^Bg_i$を用いて更新。1更新=イテレーション、全データを一巡=エポック。 | $B$はメモリと汎化のトレードオフ（大きすぎると汎化が落ちることも）。 |
| 学習率 | learning rate (η) | 更新幅。大きすぎると発散・発振、小さすぎると収束が遅い。ステップ/指数/コサイン/Warmupなどのスケジュールで調整。 | 局所適応（Adam系）でもベース学習率のチューニングは重要。 |
| モーメンタム | momentum | $v\leftarrow \beta v+(1-\beta)g$、$w\leftarrow w-\eta v$で慣性を付与して谷に沿って加速。Nesterovは$g$を先読み点で計算して過剰更新を抑える。 | 一般に$\beta\approx0.9$。最適化の初期段階で効果が大きい。 |
| RMSprop | RMSprop | 勾配二乗の移動平均でスケーリング。$s\leftarrow \rho s+(1-\rho)g^2$、$w\leftarrow w-\eta\,g/\sqrt{s+\varepsilon}$。非定常損失に強い。 | 典型設定$\rho\approx0.9,\,\varepsilon\approx10^{-8}$。 |
| Adam/AdamW | Adam / AdamW | 一次/二次モーメント推定。$m\leftarrow\beta_1m+(1-\beta_1)g,\ v\leftarrow\beta_2v+(1-\beta_2)g^2$、バイアス補正後$w\leftarrow w-\eta\,\hat m/(\sqrt{\hat v}+\varepsilon)$。AdamWは重み減衰を勾配から分離。 | 既定は$\beta_1=0.9,\beta_2=0.999,\varepsilon=10^{-8}$。汎化に難が出る場合はSGD+Momentumへ移行や重み減衰の見直し。 |
| 早期終了 | early stopping | 検証損失の改善停止を検知して学習を打ち切る。過学習を抑え、計算資源も節約。ベストエポックの重みを保存して採用。 | patience/最小改善幅/復帰条件を明確化。スケジュールと併用。 |
| 鞍点 | saddle point | 勾配は0だが最小ではない停留点。高次元では局所極小よりも鞍点が支配的。 | ノイズ（SGD）、モーメンタム、適応学習率で脱出性が向上。二階情報（Hessianの符号）で識別。 |
